name: Actualizar resultados loter√≠a

on:
  schedule:
    # Evita el pico del :00; corre cada 10 min exactos (UTC)
    - cron: "2,12,22,32,42,52 * * * *"
  workflow_dispatch:

permissions:
  contents: write

concurrency:
  group: scraper-production
  cancel-in-progress: true   # evita runs solapados que causan conflictos

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    env:
      TZ: "America/Santo_Domingo"

    steps:
      - name: Checkout repo (main)
        uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0

      - name: Timestamps (UTC y RD)
        run: |
          echo "github.event_name=${{ github.event_name }}"
          echo "github.event.schedule=${{ github.event.schedule }}"
          date -u +"UTC  %Y-%m-%d %H:%M:%S"
          TZ="America/Santo_Domingo" date +"RD   %Y-%m-%d %H:%M:%S"

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-playwright-

      - name: Instalar dependencias Python (incluye Playwright)
        run: |
          set -euxo pipefail
          python -V
          pip install --upgrade pip
          pip install playwright beautifulsoup4 requests google-auth google-auth-httplib2 google-auth-oauthlib
          if [ -f scraper/requirements.txt ]; then pip install -r scraper/requirements.txt; fi

      - name: Instalar Playwright + Chromium (usa cache si existe)
        run: |
          set -euxo pipefail
          python -m playwright install --with-deps chromium

      - name: Ejecutar tu scraper (Xvfb) + FCM por secret
        env:
          FCM_SERVICE_ACCOUNT_JSON: ${{ secrets.FCM_SERVICE_ACCOUNT_JSON }}
        run: |
          set -euxo pipefail
          xvfb-run -a python scraper/main.py
          ls -la
          echo "Scraper terminado."

      - name: Publicar HOY desde TU JSON ORIGINAL (todos los campos) y desactivar Jekyll
        run: |
          python - << 'PY'
          import json, pathlib, datetime, zoneinfo
          TZ = zoneinfo.ZoneInfo("America/Santo_Domingo")
          def hoy_str(): return datetime.datetime.now(TZ).date().isoformat()

          root_json = pathlib.Path("resultados_combinados.json")
          orig_json = pathlib.Path("scraper/data/resultados_combinados.json")
          src = None
          if orig_json.exists() and root_json.exists():
              src = orig_json if orig_json.stat().st_mtime >= root_json.stat().st_mtime else root_json
          elif orig_json.exists():
              src = orig_json
          elif root_json.exists():
              src = root_json

          outdir = pathlib.Path("docs"); outdir.mkdir(parents=True, exist_ok=True)
          (outdir / ".nojekyll").write_text("", encoding="utf-8")

          if not src:
              payload = {"generado": datetime.datetime.utcnow().isoformat()+"Z", "resultados":[]}
              (outdir/"resultados_combinados.json").write_text(json.dumps(payload, separators=(",",":")), encoding="utf-8")
              (outdir/"status.json").write_text(json.dumps({
                  "ok": False, "message": "Sin datos del scraper",
                  "generated_at_utc": payload["generado"]
              }, separators=(",",":")), encoding="utf-8")
              print("No se encontr√≥ resultados_combinados.json"); raise SystemExit(0)

          data = json.loads(src.read_text(encoding="utf-8"))

          # Sincroniza hist√≥rico en ambas rutas (id√©ntico a lo que gener√≥ tu scraper)
          for target in (root_json, orig_json):
              target.parent.mkdir(parents=True, exist_ok=True)
              target.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")

          # No filtramos claves; copiamos cada objeto tal cual
          items = data if isinstance(data, list) else data.get("resultados", [])
          hoy = hoy_str()
          solo_hoy = [it for it in items if str(it.get("fecha","")) == hoy]

          payload = {"generado": datetime.datetime.utcnow().isoformat()+"Z", "resultados": solo_hoy}
          (outdir/"resultados_combinados.json").write_text(json.dumps(payload, ensure_ascii=False, separators=(",",":")), encoding="utf-8")
          (outdir/"status.json").write_text(json.dumps({
              "ok": True, "message": f"HOY {hoy}: {len(solo_hoy)} resultados",
              "generated_at_utc": payload["generado"]
          }, separators=(",",":")), encoding="utf-8")
          print(f"Publicados HOY {hoy}: {len(solo_hoy)} (con todas las claves intactas)")
          PY

      - name: Subir artefactos de depuraci√≥n
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-scrape
          path: |
            resultados_combinados.json
            scraper/data/resultados_combinados.json
            debug_*.html
          if-no-files-found: ignore

      # üöß Guardrail: aborta si por error hay workflows en el stage
      - name: Guardrail - no commitear workflows
        run: |
          set -e
          if git diff --name-only | grep -qE '^\.github/workflows/'; then
            git checkout -- .github/workflows
          fi
          if git diff --cached --name-only | grep -qE '^\.github/workflows/'; then
            echo "::error::Se detectaron workflows en el stage. Abortando push."
            exit 1
          fi

      # ‚úÖ Push fast-forward: sin merges ni rebases; solo datos/p√°ginas; con retry
      - name: Commit & push (fast-forward, solo datos)
        run: |
          set -euxo pipefail

          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          # Alinear base con origin/main y aplicar SOLO salidas de datos/p√°ginas
          git fetch origin main
          git reset --soft origin/main

          # A√±ade SOLO estos archivos (incluye ambos nombres por compatibilidad)
          git add -f docs/resultados.json docs/resultados_combinados.json docs/status.json docs/.nojekyll \
                  resultados_combinados.json scraper/data/resultados_combinados.json || true

          # Si no hay cambios, salir
          if git diff --cached --quiet; then
            echo "Nada que commitear"
            exit 0
          fi

          git commit -m "chore: actualizar feed ($(date -u +'%Y-%m-%dT%H:%M:%SZ'))"

          # Push directo a main; si alguien se adelant√≥, reintenta una vez
          if ! git push origin HEAD:main; then
            git fetch origin main
            git reset --soft origin/main
            git add -f docs/resultados.json docs/resultados_combinados.json docs/status.json docs/.nojekyll \
                    resultados_combinados.json scraper/data/resultados_combinados.json || true
            git commit -m "chore: actualizar feed (retry $(date -u +'%Y-%m-%dT%H:%M:%SZ'))" || true
            git push origin HEAD:main
          fi

      - name: Marcar error en status.json (si falla)
        if: failure()
        run: |
          python - << 'PY'
          import json, datetime, pathlib
          outdir = pathlib.Path("docs"); outdir.mkdir(parents=True, exist_ok=True)
          (outdir / ".nojekyll").write_text("", encoding="utf-8")
          (outdir/"status.json").write_text(json.dumps({
              "ok": False,
              "message": "Fallo en scraper/normalizaci√≥n o push",
              "generated_at_utc": datetime.datetime.utcnow().isoformat()+"Z"
          }, separators=(",",":")), encoding="utf-8")
          PY
