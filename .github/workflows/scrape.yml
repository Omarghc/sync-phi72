name: Actualizar resultados lotería

on:
  schedule:
    # Evita el pico del :00; corre cada 10 min exactos (UTC)
    - cron: "2,12,22,32,42,52 * * * *"
  workflow_dispatch: {}
  repository_dispatch:
    types: [kick]

permissions:
  contents: write

concurrency:
  group: scraper-production
  cancel-in-progress: true

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    env:
      TZ: "America/Santo_Domingo"
      TARGET_API_PATH: "docs/resultados_combinados.json"  # ÚNICO archivo a publicar

    steps:
      - name: Checkout repo (main)
        uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0

      - name: Timestamps (UTC y RD)
        run: |
          echo "github.event_name=${{ github.event_name }}"
          echo "github.event.schedule=${{ github.event.schedule }}"
          date -u +"UTC  %Y-%m-%d %H:%M:%S"
          TZ="America/Santo_Domingo" date +"RD   %Y-%m-%d %H:%M:%S"

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-playwright-

      - name: Instalar dependencias Python (incluye Playwright)
        run: |
          set -euxo pipefail
          python -V
          pip install --upgrade pip
          pip install playwright beautifulsoup4 requests google-auth google-auth-httplib2 google-auth-oauthlib
          if [ -f scraper/requirements.txt ]; then pip install -r scraper/requirements.txt; fi

      - name: Instalar Playwright + Chromium (usa cache si existe)
        run: |
          set -euxo pipefail
          python -m playwright install --with-deps chromium

      - name: Ejecutar tu scraper (Xvfb) + FCM por secret
        env:
          FCM_SERVICE_ACCOUNT_JSON: ${{ secrets.FCM_SERVICE_ACCOUNT_JSON }}
        run: |
          set -euxo pipefail
          xvfb-run -a python scraper/main.py
          echo "Scraper terminado."

      # ⬇️ Publica SOLO en docs/resultados_combinados.json (histórico completo; merge + dedupe)
      - name: Publicar histórico en $TARGET_API_PATH (merge + dedupe)
        run: |
          python - <<'PY'
          import json, pathlib, datetime, zoneinfo
          TZ = zoneinfo.ZoneInfo("America/Santo_Domingo")
          now_utc = datetime.datetime.utcnow().isoformat()+"Z"

          target = pathlib.Path("${{ env.TARGET_API_PATH }}")  # docs/resultados_combinados.json
          root_a = pathlib.Path("resultados_combinados.json")
          root_b = pathlib.Path("scraper/data/resultados_combinados.json")

          sources = [p for p in (root_a, root_b) if p.exists()]
          src = max(sources, key=lambda p: p.stat().st_mtime, default=None)
          new_items = []
          if src:
            try:
              obj = json.loads(src.read_text(encoding="utf-8"))
              new_items = obj if isinstance(obj, list) else obj.get("resultados", [])
            except Exception:
              new_items = []

          old_items = []
          if target.exists():
            try:
              obj = json.loads(target.read_text(encoding="utf-8"))
              old_items = obj if isinstance(obj, list) else obj.get("resultados", [])
            except Exception:
              old_items = []

          def key_nohora(r):
            return (str(r.get("loteria","")).strip(),
                    tuple(r.get("numeros") or []),
                    str(r.get("fecha","")).strip())

          index = { key_nohora(r): i for i, r in enumerate(old_items) }
          merged = list(old_items)

          for r in new_items:
            k = key_nohora(r)
            if k in index:
              i = index[k]
              old = merged[i]
              h_old = (old.get("hora") or "").strip()
              h_new = (r.get("hora")  or "").strip()
              if (not h_old) and h_new:
                merged[i] = r
            else:
              index[k] = len(merged)
              merged.append(r)

          target.parent.mkdir(parents=True, exist_ok=True)
          payload = {"generado": now_utc, "resultados": merged}
          target.write_text(json.dumps(payload, ensure_ascii=False, separators=(",",":")), encoding="utf-8")
          print(f"[API] Escrito {target} con TOTAL={len(merged)} registros (leídos {len(new_items)} nuevos)")
          PY

      - name: Guardrail - no commitear workflows
        run: |
          set -e
          if git diff --name-only | grep -qE '^\.github/workflows/'; then
            git checkout -- .github/workflows
          fi
          if git diff --cached --name-only | grep -qE '^\.github/workflows/'; then
            echo "::error::Se detectaron workflows en el stage. Abortando push."
            exit 1
          fi

      - name: Commit & push (solo $TARGET_API_PATH)
        run: |
          set -euxo pipefail
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --global --add safe.directory "$GITHUB_WORKSPACE"

          git fetch origin main
          git reset --soft origin/main

          git add -f "${{ env.TARGET_API_PATH }}" || true

          if git diff --cached --quiet; then
            echo "Nada que commitear"
            exit 0
          fi

          git commit -m "chore: actualizar API ($(date -u +'%Y-%m-%dT%H:%M:%SZ'))"
          if ! git push origin HEAD:main; then
            git fetch origin main
            git reset --soft origin/main
            git add -f "${{ env.TARGET_API_PATH }}" || true
            git commit -m "chore: actualizar API (retry $(date -u +'%Y-%m-%dT%H:%M:%SZ'))" || true
            git push origin HEAD:main
          fi
